{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_highest-grossing_films'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the main table (first wikitable)\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "rows = table.find_all('tr')[1:]  # Skip header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x16df561eec0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Database setup\n",
    "conn = sqlite3.connect('films.db')\n",
    "c = conn.cursor()\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS films\n",
    "             (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "              title TEXT NOT NULL,\n",
    "              release_year INTEGER,\n",
    "              directors TEXT,\n",
    "              box_office REAL,\n",
    "              countries TEXT)''')\n",
    "\n",
    "# Useful for multiple runs (debugging in my case)\n",
    "c.execute('DELETE FROM films')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Film Data Scraper and Database Inserter\n",
    "\n",
    "This code scrapes film details from Wikipedia pages and inserts the cleaned data into a SQLite database. The process involves the following steps:\n",
    "\n",
    "1. **Web Scraping**: \n",
    "   - For each film, we extract details such as the title, box office revenue, release year, director(s), and country(ies).\n",
    "   - If a film has a link to its dedicated Wikipedia page, additional details like directors and countries are scraped from that page.\n",
    "\n",
    "2. **Data Cleaning**:\n",
    "   - Box office revenue is converted to a numeric value by removing dollar signs (`$`), commas (`,`), and references to other Wikipedia pages (`[1]` for example).\n",
    "   - Director names and countries are extracted as lists and stored as JSON strings.\n",
    "\n",
    "3. **Database Insertion**:\n",
    "   - The cleaned data is inserted into a SQLite database table named `films` with columns: `title`, `release_year`, `directors`, `box_office`, and `countries`.\n",
    "\n",
    "### Key Functions\n",
    "\n",
    "- **`scrape_film_details(film_url)`**:\n",
    "  - Extracts director(s) and country(ies) from the film's Wikipedia page.\n",
    "  - Handles cases where there are multiple directors or countries listed.\n",
    "  - Returns director(s) and country(ies) as JSON-encoded strings.\n",
    "\n",
    "- **Main Loop**:\n",
    "  - Iterates through rows of a table, extracts relevant data, and calls the scraper function if a film link exists.\n",
    "  - Inserts the cleaned data into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_film_details(film_url):\n",
    "    try:\n",
    "        response = requests.get(film_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract director(s)\n",
    "        directors = []\n",
    "        director_th = soup.find('th', string='Directed by')\n",
    "        if director_th:\n",
    "            director_td = director_th.find_next('td')\n",
    "            # Check if directors are in a plainlist (multiple directors)\n",
    "            plainlist = director_td.find('div', class_='plainlist')\n",
    "            if plainlist:\n",
    "                for li in plainlist.find_all('li'):\n",
    "                    for sup in li.find_all('sup'):\n",
    "                        sup.extract()\n",
    "                    director_name = li.get_text(strip=True)\n",
    "                    if director_name:\n",
    "                        directors.append(director_name)\n",
    "            else:\n",
    "                # Single director case\n",
    "                director_name = director_td.get_text(strip=True)\n",
    "                if director_name:\n",
    "                    directors.append(director_name)\n",
    "        \n",
    "        # Extract country\n",
    "        countries = []\n",
    "        country_th = soup.find('th', string='Country') or soup.find('th', string='Countries')\n",
    "        if country_th:\n",
    "            country_td = country_th.find_next('td')\n",
    "            # Check if the label is \"Countries\" (multiple countries)\n",
    "            if country_th.get_text(strip=True) == 'Countries':\n",
    "                # Extract the first country from the list\n",
    "                country_list = country_td.find('ul')\n",
    "                if country_list:\n",
    "                    for country in country_list.find_all('li'):\n",
    "                        for sup in country.find_all('sup'):\n",
    "                            sup.extract()\n",
    "                        if country:\n",
    "                            countries.append(country.get_text(strip=True))\n",
    "            else:\n",
    "                # Single country case\n",
    "                countries.append(country_td.get_text(strip=True))\n",
    "        \n",
    "        return json.dumps(directors), json.dumps(countries)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {film_url}: {e}\")\n",
    "        return json.dumps([]), json.dumps([])\n",
    "\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all(['th', 'td'])\n",
    "    title_cell = cols[2]\n",
    "    title = title_cell.get_text(strip=True)\n",
    "\n",
    "    box_office_cell = cols[3]\n",
    "    for sup in box_office_cell.find_all('sup'):\n",
    "        sup.extract()\n",
    "    box_office = float(box_office_cell.get_text(\n",
    "        strip=True).replace('$', '').replace(',', ''))\n",
    "    release_year = int(cols[4].get_text(strip=True))\n",
    "\n",
    "    film_link = title_cell.find('a')\n",
    "    directors, country = json.dumps([]), json.dumps([])\n",
    "    if film_link and film_link.get('href'):\n",
    "        directors, country = scrape_film_details(\n",
    "            'https://en.wikipedia.org' + film_link['href'])\n",
    "\n",
    "    # Insert cleaned data into the database\n",
    "    c.execute('''INSERT INTO films \n",
    "                (title, release_year, directors, box_office, countries)\n",
    "                VALUES (?, ?, ?, ?, ?)''',\n",
    "              (title, release_year, directors, box_office, country))\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the database and convert to JSON\n",
    "c.execute('SELECT * FROM films')\n",
    "records = c.fetchall()\n",
    "\n",
    "# Convert rows to a list of dictionaries\n",
    "films = []\n",
    "for record in records:\n",
    "    films.append({\n",
    "        'id': record[0],\n",
    "        'title': record[1],\n",
    "        'release_year': record[2],\n",
    "        'director': json.loads(record[3]),  # Deserialize JSON string to list\n",
    "        'box_office': record[4],\n",
    "        'countries': json.loads(record[5])  # Deserialize JSON string to list\n",
    "    })\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to JSON\n",
    "with open('./website/data/films.json', 'w') as f:\n",
    "    json.dump(films, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
